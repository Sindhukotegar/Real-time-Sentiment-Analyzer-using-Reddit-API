# -*- coding: utf-8 -*-
"""Reddit_sentimental_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jPmVBcAPQeUmTso7PVGzGdP1LdMr5H-e

# Install PRAW API:
"""

!pip3 install praw
#pip install praw #Jupyter
#conda install -c conda-forge praw #Anaconda
import praw

"""# Get Credentials:
https://old.reddit.com/prefs/apps/
"""

# Log In to App:
reddit = praw.Reddit(client_id='LchMksVUmRUeyg', client_secret='gb1XyXX-r0ycV9KKFM-ujFVNOogO_w', user_agent='Data Scraping')

"""# Hot Submissions in Subreddit:"""

subs = reddit.subreddit('datascience').hot(limit=10)

for submission in subs:
    print(submission.title)

"""# Top Submissions in the Past Year:"""

subs = reddit.subreddit('datascience').top('year')

for submission in subs:
    print(submission.title,',',submission.num_comments,',',submission.score)

for submission in reddit.subreddit("datascience").top('year'):
    print(submission.title,',',submission.num_comments,',',submission.score)

"""# Get Data into a Pandas Dataframe:"""

import pandas as pd
pd.set_option('max_colwidth', None)
df = []

subreddit = reddit.subreddit('datascience')

for post in subreddit.hot(limit=1000):
    df.append([post.title, post.score, post.url, post.num_comments, post.selftext])

df = pd.DataFrame(df,columns=['title', 'score', 'url', 'num_comments', 'body'])
df

"""Preprocessing steps for url removal."""

df5 = df.drop("url", axis=1)

df5

"""## Finding all the "Question" Topics:"""

import re

df1 = df5[df5['title'].str.contains('^.*[?]')==True]
df1.head(10)

df1.sort_values(by=['score'], inplace=True, ascending=False)
df1[['title', 'score','num_comments','body']].head(10)

pip install pyspark

pip install vaderSentiment

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
#from nltk.sentiment.vader import SentimentIntensityAnalyzer

# Initialize Spark session
spark = SparkSession.builder \
    .appName("SentimentAnalysis") \
    .getOrCreate()

# Initialize VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()


# Define function for sentiment analysis
def analyze_sentiment(text):
    sentiment_score = analyzer.polarity_scores(text)
    if sentiment_score['compound'] >= 0.05:
        return "Positive"
    elif sentiment_score['compound'] <= -0.05:
        return "Negative"
    else:
        return "Neutral"


# Pre-processing steps
df2 = df1.dropna() # Remove rows with missing text in the "body" column

# Register sentiment analysis function as a UDF
sentiment_udf = udf(analyze_sentiment, StringType())

# Create a new column
df2["posts_with_sentiment"] = df2["body"].apply(lambda text: analyzer.polarity_scores(text)["compound"])
#  can now access the sentiment score in the new column

#df.sort_values(by=['score'], inplace=True, ascending=False)
df2[['title', 'score','num_comments','body',"posts_with_sentiment"]].head(10)

#print(df2.head())

# Stop Spark session
spark.stop()

import pandas as pd

def categorize_sentiment(score):
    if score >= 0.05:
        return 'positive'
    elif score <= -0.05:
        return 'negative'
    else:
        return 'neutral'

# Apply the categorize_sentiment function to create a new column
df2['sentiment_category'] = df2['posts_with_sentiment'].apply(categorize_sentiment)

df2.head()

df2.to_csv('redditsindhu.csv', index=False)